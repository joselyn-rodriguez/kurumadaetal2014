<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Analyses of Kurumada et al (2014) - “Is it or isn’t it: Listeners make rapid use of prosody to infer speaker meanings”</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joselyn Rodriguez" />
    <script src="libs/header-attrs-2.6/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Analyses of Kurumada et al (2014) - “Is it or isn’t it: Listeners make rapid use of prosody to infer speaker meanings”
## Online Presentation Assignment
### Joselyn Rodriguez
### Rutgers University
### 2021/04/05 (updated: 2021-04-07)

---







# Introduction and background

- It's often assumed in psycholinguistics that pragmatic inference is slower and more resource intensive than other aspects of sentence processing. 
- BUT more recent studies suggest that listeners are actually able to use contextual information to form expectations which can facilitate pragmatic inference
--

- ### Sedivy et al (1999)
  - prenominal adj. facilitates reference resolution when a contrasting item is present 
- ### Watson et al (2008)
  - rapid online generation of pragmatic expectation based on visually represented contrasts for contrastive interpretation 

---
# Introduction and background 

## issues with these studies?

.pull-left[- relevant contrast was _explicitly_ mentioned prior in the discourse. 
   - difficult to generalize findings to cases hwere context contrast set is determined online (not based on previous explicit mention)
- solution?
  - visual word experiment using _it looks like an X_ construction
    - one using declarative prosodic contour (noun-focus)
    - one using contrastive L + H* on verb with rising L-H% boundary tone (Verb-focus prosody)
]

.pull-right[
&lt;img src="images/prosody.png" width="1181" /&gt;
]
    

---
# Introduction and background 
## questions addressed in this paper:

1. can listeners construct a contrast pair based on prosodic information? 
--
    - (YES)
--
2. is prosodic information integrated incrementally? 
--
    - (YES for the most part)
--
3. does the interpretation of LOOKS &lt;sub&gt;L+H*&lt;/sub&gt; involve a contextually-supported inference
--
    - (YES) 


---

# (Quick) Mehods
  
## Stimuli

.pull-left[
- 16 imageable hi-frequency bi-syllabic nouns with initial frame
- native speaker recorded two tokens of each tiem with Noun-focus and Verb-focus prosody
- 44 filler items
  - the filler items (other than the "it looks like..") were unambiguous in order to show that the speaker is generally cooperative
- there were 60 4-pic displays w/16 critical and 44 filler
  - half the critical trials were 1-contrast 
  - half were 2-contrast
]

.pull-right[
&lt;img src="images/stim.png" width="1424" /&gt;
&lt;img src="images/morestims.png" width="1005" /&gt;
]


---
# Predictions

## - if contrastive accent is interpreted incrementally....
  - in the 1-contrast display, there should be an increase in fixations to contrast set after LOOKS&lt;sub&gt;L+H*&lt;/sub&gt; and earlier gaze shifts to the non-prototypical target 
  
## - if using a context-*independent* heuristic....
  - there shouldn't be an effect of dispaly type because the atypical prosodic contour should shift the gaze to the atypical representation (non-prototypical and less-nameable distractor) with same time-course regardless of if it's 1 or 2 contrast


---

# Results

## dependent measure

- three main measures: picture choice, proportion of fixations to alternatives within display, mouse-clicking RT

## method of analysis?

- Mutli-level generalized linear regression models

---
# Results
## Picture choice

- unambiguous filler trials: 96% accuracy 
- critical trials
  - noun-focus prosody: 65.5% of trials selected prototypical target picture 
  - verb-focus prosody: 25.5% of trials selected prototypical target picture
- no further analyses here
  
---

# Results
## Eye-movements
- proportion of fixations to prototypical vs. non-prototypical pictures in 1-contrast and 2-contrast displays 
- 1 contrast displays
  - verb-focus prosody elicited more fixations to the non-prototypical target prior to the onset of the last noun
- 2 contrast displays
  - fixations in non-prototypical targets in response to verb-focus prosody and prototypical targets in response to noun-focus prosody began to increase 200ms after noun-onset 
  
.pull-justify[&lt;img src="images/eyemovres.png" width="2352" /&gt;]

---
# Results
## Eye-movements
### Model 1

.pull-left[
  - DV: logit-transformed log-odds ratios of fixations to either *members of the target contrast (e.g., zebra &amp; okapi) set vs. all pictures*
  - predictors: 
      - prosody condition (Noun-focus vs. Verb focus)
      - display type (1- vs. 2-contrast)
      - trial number
  - random effects structure:
          - random intercepts and slopes for prosody and display type by participants and prosody, display type, and trial number by items.
            - this accounts for possible variation in responses per participants and depending on prosody condition as well as items themselves. 
]
.pull-right[&lt;img src="images/model1.png" width="1133" /&gt;
- note the main effect of prosody and interaction `prosody:display-type` are significant
]

---
# Results
## Eye-movements
### Model 1
- why this analysis?
    - to see if the contrastive accent on the verb triggered more fixations to contrast-set members (e.g., zebra vs. okapi) in 1-contrast trials which would indicate that prosodic contours are interpreted with respect to visually-represented contrasts
    - they used a Multi-level generalized linear regression because...
      - "multi-level": they wanted to account for grouping factors through the use of random slopes and intercepts
      - "generalized": the DV was the *proportion* of fixations to target contrast set vs. all pictures
            - this was transformed using the logit function

---
# Results 
## Eye-movements
### Model(s) 2 

.pull-left[
- DV: logit-transformed log-odds ratios of proportion fixations to either *non-prototypical targets vs. fixations to both target pictures*
    - predictors:
        - prosody condition (Noun-focus vs. Verb focus)
        - display type (1- vs. 2-contrast)
    - random effects structure:
          - random intercepts and slopes for prosody and display type by participants
and prosody, display type, and trial number by items
]

.pull-right[
&lt;img src="images/model2.png" width="1128" /&gt;
- note that prosody condition is significant
- trial number &amp; it's interactions not significant
  - although I don't see this in the model output above...?
]

---
# Results 
## Eye-movements
### Model(s) 2 

- subsequent models that analyzed the display types separately (whose output is not displayed in the paper) additionally showed...  
  - 1 contrast displays
      - when using Verb-focus prosody, there was a significant bias toward the non-prototypical target
          -  `\(\beta\)` = .95, t = 3.07, p &lt; 0.005
  - 2 contrast displays
      - prosody was not a significant predictor 
          - in other  words, participants weren't necessarily more likely to likely to look at the prototypical target. Instead, participants were just generally more likely to still look at the non-prototypical target.
  
---
# Results
## Eye-movements

.pull-left[
- so are people just using a hueristic or not?
- given that in  1-contrast trials, participants fixated the target contrast set  more than the less-nameable (still odd), it suggests that the interpertation was derived wrt contrast-set membership instead of just visual typicality. 
- you can't really draw any conclusions from the 2-contrast display other than the non-target prototypical having the highest proportion of looks (which is unexpected)
]

.pull-right[&lt;img src="images/meanfixprop.png" width="1931" /&gt;]

---
# Results
## Mouse-click RT

- RTs were calculated by: time of picture selection - time utterance ended
- RTs were log-transformed
    - this is fairly standard from what I can tell in the literature
---
# Results
## Mouse-click RT

- analyzed using another linear model although the details here are left out. 
-  generally, effect of prosody on RT was dependent on whether the selected picture was prototypical or non-prototypical
    - `\(\beta\)` = .509, t = 2.94, p &lt; .005
--
- trials w/Noun-focus prosody
    - RTs were faster when the prototypical was selected vs. non-prototypical
        -  `\(\beta\)` = .272, t = 3.20, p &lt; .005
--
- trials w/Verb-focus prosody
    - RTs were slower for prototypical vs. non-prototypical targets
    - this is just a trend though and it didn't reach significance
        - `\(\beta\)` = .201, t = 1.10, p &gt; .10
    - RTs were slower overall for Verb-focus prosody
        - `\(\beta\)` = .242, t = 2.09, p &lt; .05
--
- overall, these data would have been consistent with delayed pragmatic inference unlike the eye-tracking data
    - hence, the "for the most part"

---
# Conclusions

- Overall, this study showed...

1. that listeners can construct a contrast pair based on prosodic information? 
2. prosodic information is integrated incrementally, but that there is an inconsitency between the eye-tracking data and the mouse-clicks. 
      - given the fact that this is an asymmetry that is seen in other parts of the literature, perhaps it's not entirely surprising. It could be prosodic information is processed in parallel to other segmental information, but that integration itself may be taking longer. 
          - e.g., discussion in Reinisch and Sjerps (2013), Cho et al (2007), Brown et al (2011)
3. it does look like the interpretation of LOOKS &lt;sub&gt;L+H*&lt;/sub&gt; involves a contextually-supported inference since we didn't see the generalization of looks towards atypical images as predicted by the heuristic

---

# Appropriateness of Analysis
## GLMMs
- Using a GLMM is generally what is done in the field as far as I can tell, so this wasn't unusual in terms of choice of analysis
- However, it is the case that many of the analyses using a linear regression are moving towards Bayesian methods, but this paper was published in 2014 so it's not surprising that they use a frequentist model.
- using a logit-transformation for porportion of fixation is also not unusual
- altogether, I think that their analysis was both appropriate and their explanations and interpretations were accurate
- the next slide, however, will discuss more general comments regarding their analysis 

---

# Presentation of Results
## General comments
- They did a good job of displaying their results for the first two models for the  most part
    - although I would have liked to have actually seen more information about their random effects structure
    - I also noticed that in their second model the graph didn't actually display the trial number even though they reference it
        - I'm not sure if this is a mistake on their part or if I am interpreting it incorrectly
- They use a random effects structure but only briefly mention this in the description above the table for their first model.
    - They also only describe how they determined what random effects they used in a footnote that would be easily missed. 
-  Finally, there is also no mention of how the variables were coded at all and no access to data/analyses to check
    - although this (sadly) usually the case
    

---
# What I liked/disliked

- I liked that the analyses were fairly straight to the point, but at the same time I didn't like that a lot of information was glossed over because of this. 
- I also would have liked a possible interpretation for the null finding for the 2-contrast trials
    - but since this is a brief article, it's not surprising that it is not discussed here
    
---
# Citations

&lt;p&gt;&lt;cite&gt;Brown, M., A. P. Salverda, L. C. Dilley, et al.
(2011).
&amp;ldquo;Expectations from preceding prosody influence segmentation in online sentence processing&amp;rdquo;.
En.
In: &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 18.6, pp. 1189&amp;ndash;1196.
ISSN: 1531-5320.
DOI: &lt;a href="https://doi.org/10.3758/s13423-011-0167-9"&gt;10.3758/s13423-011-0167-9&lt;/a&gt;.
URL: &lt;a href="https://doi.org/10.3758/s13423-011-0167-9"&gt;https://doi.org/10.3758/s13423-011-0167-9&lt;/a&gt; (visited on Apr. 08, 2021).&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;Cho, T., J. M. McQueen, and E. A. Cox
(2007).
&amp;ldquo;Prosodically driven phonetic detail in speech processing: The case of domain-initial strengthening in English&amp;rdquo;.
En.
In: &lt;em&gt;Journal of Phonetics&lt;/em&gt; 35.2, pp. 210&amp;ndash;243.
ISSN: 0095-4470.
DOI: &lt;a href="https://doi.org/10.1016/j.wocn.2006.03.003"&gt;10.1016/j.wocn.2006.03.003&lt;/a&gt;.
URL: &lt;a href="https://www.sciencedirect.com/science/article/pii/S0095447006000167"&gt;https://www.sciencedirect.com/science/article/pii/S0095447006000167&lt;/a&gt; (visited on Apr. 08, 2021).&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;Kurumada, C., M. Brown, S. Bibyk, et al.
(2014).
&amp;ldquo;Is it or isn’t it: Listeners make rapid use of prosody to infer speaker meanings&amp;rdquo;.
En.
In: &lt;em&gt;Cognition&lt;/em&gt; 133.2, pp. 335&amp;ndash;342.
ISSN: 00100277.
DOI: &lt;a href="https://doi.org/10.1016/j.cognition.2014.05.017"&gt;10.1016/j.cognition.2014.05.017&lt;/a&gt;.
URL: &lt;a href="https://linkinghub.elsevier.com/retrieve/pii/S0010027714001061"&gt;https://linkinghub.elsevier.com/retrieve/pii/S0010027714001061&lt;/a&gt; (visited on Mar. 01, 2021).&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;Reinisch, E. and M. J. Sjerps
(2013).
&amp;ldquo;The uptake of spectral and temporal cues in vowel perception is rapidly influenced by context&amp;rdquo;.
En.
In: &lt;em&gt;Journal of Phonetics&lt;/em&gt; 41.2, pp. 101&amp;ndash;116.
ISSN: 0095-4470.
DOI: &lt;a href="https://doi.org/10.1016/j.wocn.2013.01.002"&gt;10.1016/j.wocn.2013.01.002&lt;/a&gt;.
URL: &lt;a href="https://www.sciencedirect.com/science/article/pii/S0095447013000053"&gt;https://www.sciencedirect.com/science/article/pii/S0095447013000053&lt;/a&gt; (visited on Apr. 08, 2021).&lt;/cite&gt;&lt;/p&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
